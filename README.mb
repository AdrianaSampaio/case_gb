# Desafio Técnico: Especialista de Dados I — Crédito e Cobrança PF

## Questão 1 — Pipeline de Dados + Geração de Indicadores  
#### Arquitetura Completa: RAW → TRUSTED → VALIDAÇÕES → INDICADORES


Este diretório reúne todos os entregáveis desenvolvidos para responder à **Questão 1**, organizada em duas partes:

1. **Parte 1 — Pipeline RAW → TRUSTED e Validações de Qualidade**  
2. **Parte 2 — Geração de Indicadores Analíticos via SQL**

O objetivo é demonstrar o fluxo completo de uma **mini arquitetura de dados**, desde ingestão bruta até criação de métricas estratégicas.

---

### Parte 1 — Pipeline RAW → TRUSTED  
#### Ingestão, Estruturação e Validação da Base de Usuários

A API pública fornece 30 usuários em JSON. Esses dados passam por três etapas fundamentais:

- **RAW** (armazenamento bruto, íntegro)  
- **TRUSTED** (dados limpos e estruturados)  
- **Validação** (conferências automáticas de qualidade)

---

#### Arquivos incluídos (Parte 1)

| Arquivo                      | Descrição                                                              |
|------------------------------|------------------------------------------------------------------------|
| **etl_users.py**             | Ingestão da API, criação do banco SQLite e transformação RAW → TRUSTED |
| **etl_users_validations.py** | Regras de validação aplicadas sobre a camada TRUSTED                   |
| **users.db**                 | Banco SQLite contendo tabelas RAW e TRUSTED                            |

---

#### Arquitetura do Pipeline

```
          +---------------------------+
          |         API Pública       |
          |    (200 usuários JSON)    |
          +-------------+-------------+
                        |
                        v
              [ ETAPA 1 - RAW ]
+-----------------------------------------------------+
|  RAW: dados brutos, íntegros e rastreáveis          |
|  Estrutura:                                         |
|   - user_id                                         |
|   - raw_json (conteúdo em JSON)                     |
+-----------------------------------------------------+
                            |
                            v
               [ ETAPA 2 - TRUSTED ]
+------------------------------------------------------+
| TRUSTED: base limpa e padronizada                    |
| Tabelas: users, address, professional_info, bank     |
| Atributos normalizados e estruturados                |
+------------------------------------------------------+
                            |
                            v
              [ ETAPA 3 - VALIDAÇÕES ]
+------------------------------------------------------+
| Regras verificadas:                                  |
| - IDs duplicados                                     |
| - Usuários sem endereço                              |
| - Usuários sem info profissional                     |
| - Distribuição por gênero                            |
| - Distribuição por departamento                      |
| - Regras de aplicabilidade (ex.: TI < 40 anos)       |
+------------------------------------------------------+
```

---

#### Fluxo Geral do Processo

```
┌─────────────┐     ingestão      ┌─────────────┐     transformação    ┌─────────────┐
│    API      │ ───────────────►  │     RAW     │ ───────────────────► │   TRUSTED   │
└─────────────┘                   └─────────────┘                      └─────────────┘
                                              \                              
                                               \                             validações
                                                \──────► etl_users_validations.py
```

---

### Parte 2 — Indicadores SQL

Depois da criação das camadas RAW e TRUSTED, um conjunto de indicadores analíticos foi desenvolvido usando SQL, simulando uma camada **Analytics / DataMart**.

Esses indicadores permitem análises estratégicas do portfólio de usuários.

---

#### Arquivo incluído (Parte 2)

| Arquivo                      | Descrição                                                                          |
|------------------------------|------------------------------------------------------------------------------------|
| **etl_users_indicators.sql** | Script SQL que cria uma tabela de indicadores agregados a partir da camada TRUSTED |

---

#### Indicadores criados
- pct_usuarios_ti -> Percentual de usuários da área de TI.  
- idade_media     -> Média de idade.  
- total_usuarios  -> Total de usuários.  
- pct_menos_30    -> Percentual de usuários com menos de 30 anos. 

---

## Questão 2 — Orquestração de Processos (Airflow)
#### Fluxo de Cobrança Crítico com Lógica de Acúmulo e Parametrização ⚙️

Esta seção apresenta a DAG Airflow desenvolvida para orquestrar o processo de atualização da "Régua de Cobrança", incorporando regras de **acúmulo de dados no final de semana** e processamento em **lote** nos dias úteis.

#### Arquivo incluído (Questão 2)

| Arquivo                                  | Descrição                                                                                                                       |
|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|
| **dag_regua_cobranca_parametrizavel.py** | Definição da DAG com lógica de **Branching** (Dia Útil vs. Fim de Semana), **3 tentativas de retry** e **tratamento de erros**. |

---

#### Lógica de Fluxo da DAG

O fluxo implementa uma estratégia de acumulação e processamento em lote para garantir que nenhuma carga no banco ocorra no final de semana, otimizando recursos:

1. **A cada 5 minutos:** Verifica se o arquivo **`pagamentos_d-1.csv`** chegou em `DIR_ORIGEM`. Se não, a DAG encerra (Short-Circuit).
2. **Processamento:** O arquivo é processado e movido para a pasta de acumulação (**`DIR_PROCESSADOS`**).
3. **Fim de Semana (Acúmulo):** O fluxo é direcionado ao `end`, deixando o arquivo acumulado em **`DIR_PROCESSADOS`**. **Nenhuma carga no DB ocorre.**
4. **Dia Útil (Lote):** O fluxo executa em lote:
    * **Carga no DB:** A tarefa lê **TODOS** os arquivos acumulados em **`DIR_PROCESSADOS`** e os envia ao banco de dados.
    * **Arquivamento Final:** Após a carga ser bem-sucedida, **TODOS** os arquivos são movidos para **`DIR_CARGA_FINAL`**.

---

#### Estrutura de Diretórios e Fluxo de Dados

| Diretório             | Função                                                                                    |
|-----------------------|-------------------------------------------------------------------------------------------|
| **`DIR_ORIGEM`**      | Entrada inicial do arquivo (`pagamentos_d-1.csv`).                                        |
| **`DIR_PROCESSADOS`** | **Acumulação/Log de Pendentes.** Fonte de dados para a carga no banco durante a semana.   |
| **`DIR_CARGA_FINAL`** | **Destino final/Histórico.** Arquivamento após a carga em lote ser concluída com sucesso. |
| **`DIR_ERROS`**       | Quarentena para arquivos que falharam após **3 tentativas** de reexecução.                |

---

#### Parametrização de Execução (Airflow Variables)

A DAG foi configurada para que parâmetros críticos possam ser alterados **sem modificar o código**, utilizando as Variáveis do Airflow (Airflow Variables):

| Variável                              | Padrão        | Descrição                                                                                                               |
|---------------------------------------|---------------|-------------------------------------------------------------------------------------------------------------------------|
| **`regua_cobranca_retries`**          | `3`           | Número de vezes que a tarefa deve tentar reexecutar em caso de falha.                                                   |
| **`regua_cobranca_retry_delay_min`**  | `5`           | Intervalo de espera, em minutos, entre as tentativas (retries).                                                         |
| **`regua_cobranca_dias_envio_banco`** | `"1,2,3,4,5"` | Dias da semana (ISO: 1=Segunda, 7=Domingo) que devem seguir o fluxo de **carga no banco**. Outros dias apenas acumulam. |

---